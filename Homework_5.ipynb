{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3871b6b-964c-4332-a6fa-aa4ba061ebdf",
   "metadata": {},
   "source": [
    "# Chem 277B - Fall 2024 - Homework 5\n",
    "## ANN - Artificial Neural Networks\n",
    "*Submit this notebook to bCourses to receive a credit for this assignment.*\n",
    "<br>\n",
    "due: **Oct 29th 2024** \n",
    "<br>\n",
    "**Please upload both, the .ipynb file and the corresponding .pdf**<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda39810-091f-482b-a7fb-906c6da69c7f",
   "metadata": {},
   "source": [
    "## 60 Points Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad98376-0aa9-4d9a-b182-c46a251e90d6",
   "metadata": {},
   "source": [
    "The goal of this homework assignment is to understand how the complexity of the dataset and the design of the ANN (number of neurons and number of layers) with different parameters i.e. like learning rate and activation functions influence the performance of the ANN.<br>\n",
    "**Important: Only use numpy for creating the ANN, pandas for data frames if neccessary and matplotlib/ seaborn for plotting, but no further external python libraries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302b60a-2aa0-43a7-bd68-2650720d3a5e",
   "metadata": {},
   "source": [
    "**1) Create an artificial dataset, one for regression and one for classification each** <br>\n",
    "Start with a data set similar to the molecule data set we have been using earlier with a moderate number of features (say five) and about 1000 data points.<br> \n",
    "Normalize the features between 0 and 1 for better convergence and split the dataset into **training and testing set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c222ec-a21f-44d9-a5a5-5f9b2b34cdc3",
   "metadata": {},
   "source": [
    "**2) Network Design**<br>\n",
    "Use at least two hidden layers and experiment with different layer sizes. Use different activation functions, such like *Sigmoid*, *ReLU* or any other activation function of your choice.<br>\n",
    "Implement dropout between the hidden layers (e.g., randomly drop 20-30% neurons).<br>\n",
    "Add an output layer according to the optimization problem (regression vs classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6dbb7-6df6-4484-8459-e7c9279e7e0b",
   "metadata": {},
   "source": [
    "**3) Training and Optimization**<br>\n",
    "For regression, use Mean Squared Error (MSE) as the loss function. For classification, use cross entropy as loss function (you can use the codes provided in the lecture). Implement backpropagation manually for weight updates and use gradient descent for optimization. Now, train the network over multiple epochs and track the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dddee3b-f36b-4543-9c27-1c7dc93a3b80",
   "metadata": {},
   "source": [
    "**4) Evaluation**<br>\n",
    "Monitor the loss and the accuracy for the different epochs. For classification, generate a confusion chart and plot a histogramm of the different probabilities (see the lecture) at the end of the training process.<br>\n",
    "Evaluate the performance of the ANN with the test set in the same way.<br>\n",
    "<br>\n",
    "Now, experiment with different<br>\n",
    "- training to test set ratios<br>\n",
    "- different numbers of features in the data (say, $N_{feature} = 3, 5, 20, 50$)\n",
    "- features that correlate\n",
    "- different numbers of data points (say, $N_{sample} = 200, 2\\,000, 5\\,000, 10\\,000$)\n",
    "\n",
    "How does the accuracy change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6057e2-0c5b-4e79-9cae-b0afc98086f0",
   "metadata": {},
   "source": [
    "**5) Submission Requirements**<br>\n",
    "Include a short report (1-2 pages) explaining:<br>\n",
    "- your architecture choices (layers, neurons, activation functions, etc.)<br>\n",
    "- how dropout was implemented.<br>\n",
    "- training performance (loss and accuracy plots).<br>\n",
    "- results and key insights.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5664d",
   "metadata": {},
   "source": [
    "## Create artificial dataset function\n",
    "\n",
    "**1) Create an artificial dataset, one for regression and one for classification each** <br>\n",
    "Start with a data set similar to the molecule data set we have been using earlier with a moderate number of features (say five) and about 1000 data points.<br> \n",
    "Normalize the features between 0 and 1 for better convergence and split the dataset into **training and testing set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c591ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from sklearn.preprocessing import MinMaxScaler  #to normalize data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "133a942c-ae5e-437c-9c14-6ac41887b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(num_data_points, num_features):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for num in range(num_features):\n",
    "        curr_column = np.random.randint(0, 100, size = num_data_points)\n",
    "        normalized_curr_col = MinMaxScaler().fit_transform(curr_column.reshape(-1,1)).flatten()\n",
    "        df[f\"feature {num+1}\"] = normalized_curr_col.flatten()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654f519",
   "metadata": {},
   "source": [
    "## make classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "659ef627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it a classification dataframe\n",
    "\n",
    "def make_class_df(df,num_classes):\n",
    "\n",
    "    for num in range(num_classes):\n",
    "        copy_df = df.copy()\n",
    "\n",
    "        columns = df.shape[1]\n",
    "\n",
    "        feature_columns = [col for col in df.columns if not col.startswith('class')]\n",
    "\n",
    "        random_col = np.random.choice(feature_columns, 2, replace = False)\n",
    "\n",
    "        new_class = (df[random_col[0]] + df[random_col[1]] > 1).astype(int)\n",
    "\n",
    "        copy_df[f'class {num}'] = new_class\n",
    "\n",
    "        df = copy_df\n",
    "\n",
    "    return copy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0889a5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature 1</th>\n",
       "      <th>feature 2</th>\n",
       "      <th>feature 3</th>\n",
       "      <th>feature 4</th>\n",
       "      <th>feature 5</th>\n",
       "      <th>class 0</th>\n",
       "      <th>class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.373737</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.131313</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.464646</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature 1  feature 2  feature 3  feature 4  feature 5  class 0  class 1\n",
       "0     0.616162   0.808081   0.040404   0.181818   0.747475        0        0\n",
       "1     0.242424   0.262626   0.232323   0.040404   0.535354        0        0\n",
       "2     0.696970   0.686869   0.151515   0.212121   0.272727        0        0\n",
       "3     0.646465   0.373737   0.141414   0.373737   0.090909        0        0\n",
       "4     0.131313   0.020202   0.717172   0.969697   0.717172        1        1\n",
       "..         ...        ...        ...        ...        ...      ...      ...\n",
       "995   0.626263   0.000000   0.090909   0.131313   0.242424        0        0\n",
       "996   0.464646   0.646465   0.323232   0.747475   0.848485        1        1\n",
       "997   0.151515   0.121212   0.151515   0.656566   0.525253        0        1\n",
       "998   0.000000   0.676768   0.757576   0.353535   0.919192        1        1\n",
       "999   0.383838   0.858586   0.353535   0.141414   0.333333        0        0\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = create_dataset(1000, 5)\n",
    "\n",
    "test_multi_class_df = make_class_df(test_df, 2)\n",
    "\n",
    "display(test_multi_class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614aa77d",
   "metadata": {},
   "source": [
    "## make Train test split function for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c7241da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tt_split_regression(df, test_percent):\n",
    "\n",
    "    #make last feature Y\n",
    "\n",
    "    Y = df.iloc[:,-1]\n",
    "\n",
    "    X = df.iloc[:,:-1]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_percent )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989c98a",
   "metadata": {},
   "source": [
    "## Train test split function for classification df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "37ccaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_split_classification(df, test_percent):\n",
    "\n",
    "    #make last feature Y\n",
    "\n",
    "    Y = df['toxic']\n",
    "\n",
    "    X = df.drop(columns = \"toxic\")\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_percent )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a2921",
   "metadata": {},
   "source": [
    "## Create regression and classification dataframes and do train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "90539cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## create regression dataframe\n",
    "rg_df_1 = create_dataset(1000, 5)\n",
    "\n",
    "#create classificaiton dataframe\n",
    "cl_df_1 = make_class_df(rg_df_1,3)\n",
    "#display(cl_df_1)\n",
    "#print(cl_df_1['toxic'])\n",
    "\n",
    "#train test split regression dataframe\n",
    "x_train_reg, x_test_reg, y_train_reg, y_test_reg = tt_split_regression(rg_df_1, 0.3)\n",
    "\n",
    "#train test split classification dataframe\n",
    "x_train_cla, x_test_cla, y_train_cla, y_test_cla = tt_split_regression(cl_df_1, 0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd74aff",
   "metadata": {},
   "source": [
    "## Using classes from ANNIII.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7d2b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1* np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #gradients\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases  = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        self.dinputs  = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU():\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.maximum(0, inputs)\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0#ReLU derivative\n",
    "\n",
    "class Activation_Sigmoid():\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.clip(1/(1 + np.exp(-inputs)), 1e-7, 1-1e-7)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        sigm         = self.output\n",
    "        deriv        = np.multiply(sigm, (1 - sigm))#inner derivative of sigmoid\n",
    "        self.dinputs = np.multiply(deriv, dvalues)\n",
    "\n",
    "class Activation_Softmax:\n",
    "  \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values  = np.exp(inputs - np.max(inputs, axis = 1,\\\n",
    "                                      keepdims = True))#max in order to \n",
    "                                                       #prevent overflow\n",
    "        #normalizing probs (Boltzmann dist.)\n",
    "        probabilities = exp_values/np.sum(exp_values, axis = 1,\\\n",
    "                                      keepdims = True)  \n",
    "        self.output   = probabilities                                                \n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for i, (single_output, single_dvalues) in \\\n",
    "            enumerate(zip(self.output, dvalues)):\n",
    "            \n",
    "            single_output   = single_output.reshape(-1,1)\n",
    "            jacobMatr       = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            self.dinputs[i] = np.dot(jacobMatr, single_dvalues)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "     \n",
    "     def calculate(self, output, y):\n",
    "         \n",
    "         sample_losses = self.forward(output, y)\n",
    "         data_loss     = np.mean(sample_losses)\n",
    "         return(data_loss)\n",
    "    \n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss): \n",
    "\n",
    "     def forward(self, y_pred, y_true):\n",
    "         samples = len(y_pred)\n",
    "         #removing vals close to zero and one bco log and accuracy\n",
    "         y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "         \n",
    "         #now, depending on how classes are coded, we need to get the probs\n",
    "         if len(y_true.shape) == 1:#classes are encoded as [[1],[2],[2],[4]]\n",
    "             correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "         elif len(y_true.shape) == 2:#classes are encoded as\n",
    "                                    #[[1,0,0], [0,1,0], [0,1,0]]\n",
    "             correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
    "         #now: calculating actual losses\n",
    "         negative_log_likelihoods = -np.log(correct_confidences)\n",
    "         return(negative_log_likelihoods)\n",
    "         \n",
    "     def backward(self, dvalues, y_true):\n",
    "         Nsamples = len(dvalues)\n",
    "         Nlabels  = len(dvalues[0])\n",
    "         #turning labels into one-hot i. e. [[1,0,0], [0,1,0], [0,1,0]], if\n",
    "         #they are not\n",
    "         if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(Nlabels)[y_true]\n",
    "         #normalized gradient\n",
    "         self.dinputs = -y_true/dvalues/Nsamples\n",
    "\n",
    "\n",
    "\n",
    "#Creating a class as parent for softmax, loss and entropy classes. \n",
    "#Actually not neccessary, but saves code when building the ANN\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss       = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output#the probabilities\n",
    "        #calculates and returns mean loss\n",
    "        return(self.loss.calculate(self.output, y_true))\n",
    "        \n",
    "    def backward(self, dvalues, y_true):\n",
    "        Nsamples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculating normalized gradient\n",
    "        self.dinputs[range(Nsamples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs/Nsamples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    #initializing with a default learning rate of 0.01\n",
    "    def __init__(self, learning_rate = 0.01, decay = 0, momentum = 0):\n",
    "        self.learning_rate         = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay                 = decay\n",
    "        self.iterations            = 0\n",
    "        self.momentum              = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1/ (1 + self.decay*self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if we use momentum\n",
    "        if self.momentum:\n",
    "            \n",
    "            #check if layer has attribute \"momentum\"\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums   = np.zeros_like(layer.biases)\n",
    "                \n",
    "            #now the momentum parts\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates   = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases  += bias_updates\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Layer_dropout():\n",
    "    #source : https://www.youtube.com/watch?v=tkvlspCbLqo&ab_channel=Vizuara\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #generate scaled binary mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size = inputs.shape) / self.rate\n",
    "        #apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916b339",
   "metadata": {},
   "source": [
    "## Network Design\n",
    "\n",
    "Use at least two hidden layers and experiment with different layer sizes. Use different activation functions, such like *Sigmoid*, *ReLU* or any other activation function of your choice.<br>\n",
    "Implement dropout between the hidden layers (e.g., randomly drop 20-30% neurons).<br>\n",
    "Add an output layer according to the optimization problem (regression vs classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418570f4",
   "metadata": {},
   "source": [
    "## Regression ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "580e960f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature 1</th>\n",
       "      <th>feature 2</th>\n",
       "      <th>feature 3</th>\n",
       "      <th>feature 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.474747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.707071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.828283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.101010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.323232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.919192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.868687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.707071</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.929293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature 1  feature 2  feature 3  feature 4\n",
       "40    0.323232   0.727273   0.202020   0.474747\n",
       "255   0.353535   0.979798   0.505051   0.707071\n",
       "492   0.838384   0.292929   0.424242   0.828283\n",
       "991   0.787879   0.606061   0.828283   0.030303\n",
       "444   0.424242   0.616162   0.303030   0.101010\n",
       "..         ...        ...        ...        ...\n",
       "281   0.616162   0.363636   0.191919   0.323232\n",
       "142   0.343434   0.343434   0.191919   0.919192\n",
       "647   0.212121   0.535354   0.676768   0.868687\n",
       "973   0.545455   0.707071   0.141414   0.929293\n",
       "617   0.060606   0.474747   0.171717   0.818182\n",
       "\n",
       "[700 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# set up parameters\n",
    "\n",
    "Nneurons1 = 64\n",
    "Nneurons2 = 128\n",
    "Nneurons3 = 256\n",
    "\n",
    "Nepochs = 2000\n",
    "learning_rate = 0.0001\n",
    "decay = 0.001\n",
    "momentum = 0.8\n",
    "dropout_rate = 0.2\n",
    "\n",
    "Nfeatures = x_train_reg.shape[1] #columns in regression df\n",
    "display(x_train_reg)\n",
    "print(x_train_reg.shape[1])\n",
    "\n",
    "dense1 = Layer_Dense(Nfeatures, Nneurons1)\n",
    "dense2 = Layer_Dense(Nneurons1, Nneurons2)\n",
    "dense3 = Layer_Dense(Nneurons2, 1)\n",
    "\n",
    "drop1 = Layer_dropout(dropout_rate)\n",
    "drop2 = Layer_dropout(dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = Optimizer_SGD(learning_rate = learning_rate, decay = decay, momentum = momentum)\n",
    "\n",
    "ReLU1 = Activation_ReLU()\n",
    "ReLU2 = Activation_ReLU()\n",
    "\n",
    "\n",
    "loss_activation =  mean_squared_error\n",
    "\n",
    "Target = y_train_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1e90b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy (RMSE): 0.542, loss: 0.294, actual learning rate: 0.0001\n",
      "epoch: 100, accuracy (RMSE): 0.471, loss: 0.222, actual learning rate: 9.090909090909092e-05\n",
      "epoch: 200, accuracy (RMSE): 0.421, loss: 0.177, actual learning rate: 8.333333333333334e-05\n",
      "epoch: 300, accuracy (RMSE): 0.386, loss: 0.149, actual learning rate: 7.692307692307693e-05\n",
      "epoch: 400, accuracy (RMSE): 0.364, loss: 0.132, actual learning rate: 7.142857142857143e-05\n",
      "epoch: 500, accuracy (RMSE): 0.347, loss: 0.120, actual learning rate: 6.666666666666667e-05\n",
      "epoch: 600, accuracy (RMSE): 0.337, loss: 0.113, actual learning rate: 6.25e-05\n",
      "epoch: 700, accuracy (RMSE): 0.328, loss: 0.108, actual learning rate: 5.88235294117647e-05\n",
      "epoch: 800, accuracy (RMSE): 0.321, loss: 0.103, actual learning rate: 5.555555555555556e-05\n",
      "epoch: 900, accuracy (RMSE): 0.317, loss: 0.100, actual learning rate: 5.2631578947368424e-05\n",
      "epoch: 1000, accuracy (RMSE): 0.313, loss: 0.098, actual learning rate: 5e-05\n",
      "epoch: 1100, accuracy (RMSE): 0.309, loss: 0.096, actual learning rate: 4.761904761904762e-05\n",
      "epoch: 1200, accuracy (RMSE): 0.309, loss: 0.095, actual learning rate: 4.545454545454546e-05\n",
      "epoch: 1300, accuracy (RMSE): 0.310, loss: 0.096, actual learning rate: 4.347826086956523e-05\n",
      "epoch: 1400, accuracy (RMSE): 0.312, loss: 0.097, actual learning rate: 4.1666666666666665e-05\n",
      "epoch: 1500, accuracy (RMSE): 0.307, loss: 0.094, actual learning rate: 4e-05\n",
      "epoch: 1600, accuracy (RMSE): 0.307, loss: 0.094, actual learning rate: 3.846153846153846e-05\n",
      "epoch: 1700, accuracy (RMSE): 0.303, loss: 0.092, actual learning rate: 3.7037037037037037e-05\n",
      "epoch: 1800, accuracy (RMSE): 0.304, loss: 0.093, actual learning rate: 3.571428571428572e-05\n",
      "epoch: 1900, accuracy (RMSE): 0.305, loss: 0.093, actual learning rate: 3.4482758620689657e-05\n"
     ]
    }
   ],
   "source": [
    "Monitor = np.zeros((Nepochs, 3)) #stores loss, learning rate, accuracy\n",
    "\n",
    "for epoch in range(Nepochs):\n",
    "\n",
    "    '''\n",
    "    FORWARD PASS\n",
    "    --------------\n",
    "    '''\n",
    "\n",
    "    #layer 1 forward\n",
    "    dense1.forward(x_train_reg)\n",
    "    ReLU1.forward(dense1.output)\n",
    "    drop1.forward(ReLU1.output)\n",
    "\n",
    "    #layer 2 forward \n",
    "    dense2.forward(drop1.output)\n",
    "    ReLU2.forward(dense2.output)\n",
    "    drop2.forward(ReLU2.output)\n",
    "\n",
    "    #layer 3 forward\n",
    "    dense3.forward(drop2.output)\n",
    "    \n",
    "\n",
    "    #get predictions\n",
    "    Y_pred = dense3.output\n",
    "\n",
    "    #display(Y_pred)\n",
    "    #print(\"Y-pred: \",Y_pred.shape)\n",
    "    #print(\"y_train_reg: \", Target.shape)\n",
    "    \n",
    "    #reshape to 1D\n",
    "    oned_Y_pred = np.reshape(Y_pred, -1)\n",
    "    oned_Target = np.reshape(Target, -1)\n",
    "\n",
    "    loss = loss_activation(oned_Target, oned_Y_pred) #MSE\n",
    "\n",
    "    RMSE = math.sqrt(loss)\n",
    "\n",
    "    #calculate gradient of MSE (2/n(Y_pred - Target))\n",
    "    dY_pred = 2 * ( oned_Y_pred - oned_Target) / Target.shape[0]\n",
    "\n",
    "    #fix dY_pred dimensions\n",
    "    dY_pred = dY_pred.reshape(Y_pred.shape)\n",
    "    '''\n",
    "    BACKWARD PASS\n",
    "    -----------------\n",
    "    '''\n",
    "    #layer 3 backward\n",
    "    dense3.backward(dY_pred)\n",
    "\n",
    "    #layer 2 backward\n",
    "    drop2.backward(dense3.dinputs)\n",
    "    ReLU2.backward(drop2.dinputs)\n",
    "    dense2.backward(ReLU2.dinputs)\n",
    "\n",
    "    #layer 1 backward\n",
    "    drop1.backward(dense2.dinputs)\n",
    "    ReLU1.backward(drop1.dinputs)\n",
    "    dense1.backward(ReLU1.dinputs)\n",
    "\n",
    "    '''\n",
    "    Apply gradient descent\n",
    "\n",
    "    this only applies to layers with weights, therefore exclude activation layers\n",
    "    (ReLU1,2,3 and drop1,2, and 3)\n",
    "    '''\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "    Monitor[epoch, 0] = RMSE #accuracy\n",
    "    Monitor[epoch, 1] = loss #MSE\n",
    "    Monitor[epoch,  2] = optimizer.current_learning_rate\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'accuracy (RMSE): {RMSE:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'actual learning rate: {optimizer.current_learning_rate}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce79f81",
   "metadata": {},
   "source": [
    "Evaluating the fit of the regression ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f03ac715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xplot = np.arange(Nepochs)\n",
    "\n",
    "fig1, ax1 = plt.subplots(3, 1, sharex = True)\n",
    "ax1[0].plot(xplot, 100*Monitor[:,0])\n",
    "ax1[0].set_ylabel('RMSE')\n",
    "ax1[1].plot(xplot, Monitor[:,1])\n",
    "ax1[1].set_ylabel('loss')\n",
    "ax1[2].plot(xplot, Monitor[:,2])\n",
    "ax1[2].set_ylabel(r'$\\alpha$')\n",
    "ax1[2].set_xlabel('epoch')\n",
    "plt.xscale('log', base = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde60ef",
   "metadata": {},
   "source": [
    "## Classification ANN\n",
    "\n",
    "set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f4249c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "\n",
    "Nneurons1 = 64\n",
    "Nneurons2 = 128\n",
    "Nneurons3 = 256\n",
    "\n",
    "Nepochs = 2000\n",
    "learning_rate = 0.001\n",
    "decay = 0.001\n",
    "momentum = 0.8\n",
    "dropout_rate = 0.2\n",
    "\n",
    "Nfeatures = x_train_cla.shape[1] #columns in regression df\n",
    "\n",
    "dense1 = Layer_Dense(Nfeatures, Nneurons1)\n",
    "dense2 = Layer_Dense(Nneurons1, Nneurons2)\n",
    "dense3 = Layer_Dense(Nneurons2, 1)\n",
    "\n",
    "drop1 = Layer_dropout(dropout_rate)\n",
    "drop2 = Layer_dropout(dropout_rate)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = Optimizer_SGD(learning_rate = learning_rate, decay = decay, momentum = momentum)\n",
    "\n",
    "sigmoid1 = Activation_Sigmoid()\n",
    "sigmoid2 = Activation_Sigmoid()\n",
    "\n",
    "\n",
    "loss_activation =  Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "Target = y_train_cla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447a5c1",
   "metadata": {},
   "source": [
    "## Training Classification ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d89fd201",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#layer 3 output\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dense3\u001b[38;5;241m.\u001b[39mforward(drop2\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_activation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdense3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(loss_activation\u001b[38;5;241m.\u001b[39moutput, axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m loss_activation\u001b[38;5;241m.\u001b[39mbackward(loss_activation\u001b[38;5;241m.\u001b[39moutput, Target)\n",
      "Cell \u001b[0;32mIn[218], line 112\u001b[0m, in \u001b[0;36mActivation_Softmax_Loss_CategoricalCrossentropy.forward\u001b[0;34m(self, inputs, y_true)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;66;03m#the probabilities\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#calculates and returns mean loss\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[218], line 66\u001b[0m, in \u001b[0;36mLoss.calculate\u001b[0;34m(self, output, y)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate\u001b[39m(\u001b[38;5;28mself\u001b[39m, output, y):\n\u001b[0;32m---> 66\u001b[0m     sample_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     data_loss     \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(sample_losses)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(data_loss)\n",
      "Cell \u001b[0;32mIn[218], line 80\u001b[0m, in \u001b[0;36mLoss_CategoricalCrossEntropy.forward\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#now, depending on how classes are coded, we need to get the probs\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_true\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\u001b[38;5;66;03m#classes are encoded as [[1],[2],[2],[4]]\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     correct_confidences \u001b[38;5;241m=\u001b[39m \u001b[43my_pred_clipped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_true\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\u001b[38;5;66;03m#classes are encoded as\u001b[39;00m\n\u001b[1;32m     82\u001b[0m                            \u001b[38;5;66;03m#[[1,0,0], [0,1,0], [0,1,0]]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     correct_confidences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_pred_clipped\u001b[38;5;241m*\u001b[39my_true, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "Monitor_cla = np.zeros((Nepochs, 3))\n",
    "\n",
    "for epoch in range(Nepochs):\n",
    "    #layer 1 forward pass class ANN\n",
    "    dense1.forward(x_train_cla)\n",
    "    sigmoid1.forward(dense1.output)\n",
    "    drop1.forward(sigmoid1.output)\n",
    "\n",
    "    #layer 2 forward pass class ANN\n",
    "    dense2.forward(drop1.output)\n",
    "    sigmoid2.forward(dense2.output)\n",
    "    drop2.forward(sigmoid2.output)\n",
    "\n",
    "    #layer 3 output\n",
    "    dense3.forward(drop2.output)\n",
    "\n",
    "    loss = loss_activation.forward(dense3.output, Target)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis =1)\n",
    "\n",
    "    loss_activation.backward(loss_activation.output, Target)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
