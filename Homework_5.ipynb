{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3871b6b-964c-4332-a6fa-aa4ba061ebdf",
   "metadata": {},
   "source": [
    "# Chem 277B - Fall 2024 - Homework 5\n",
    "## ANN - Artificial Neural Networks\n",
    "*Submit this notebook to bCourses to receive a credit for this assignment.*\n",
    "<br>\n",
    "due: **Oct 29th 2024** \n",
    "<br>\n",
    "**Please upload both, the .ipynb file and the corresponding .pdf**<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda39810-091f-482b-a7fb-906c6da69c7f",
   "metadata": {},
   "source": [
    "## 60 Points Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad98376-0aa9-4d9a-b182-c46a251e90d6",
   "metadata": {},
   "source": [
    "The goal of this homework assignment is to understand how the complexity of the dataset and the design of the ANN (number of neurons and number of layers) with different parameters i.e. like learning rate and activation functions influence the performance of the ANN.<br>\n",
    "**Important: Only use numpy for creating the ANN, pandas for data frames if neccessary and matplotlib/ seaborn for plotting, but no further external python libraries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302b60a-2aa0-43a7-bd68-2650720d3a5e",
   "metadata": {},
   "source": [
    "**1) Create an artificial dataset, one for regression and one for classification each** <br>\n",
    "Start with a data set similar to the molecule data set we have been using earlier with a moderate number of features (say five) and about 1000 data points.<br> \n",
    "Normalize the features between 0 and 1 for better convergence and split the dataset into **training and testing set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c222ec-a21f-44d9-a5a5-5f9b2b34cdc3",
   "metadata": {},
   "source": [
    "**2) Network Design**<br>\n",
    "Use at least two hidden layers and experiment with different layer sizes. Use different activation functions, such like *Sigmoid*, *ReLU* or any other activation function of your choice.<br>\n",
    "Implement dropout between the hidden layers (e.g., randomly drop 20-30% neurons).<br>\n",
    "Add an output layer according to the optimization problem (regression vs classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6dbb7-6df6-4484-8459-e7c9279e7e0b",
   "metadata": {},
   "source": [
    "**3) Training and Optimization**<br>\n",
    "For regression, use Mean Squared Error (MSE) as the loss function. For classification, use cross entropy as loss function (you can use the codes provided in the lecture). Implement backpropagation manually for weight updates and use gradient descent for optimization. Now, train the network over multiple epochs and track the loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dddee3b-f36b-4543-9c27-1c7dc93a3b80",
   "metadata": {},
   "source": [
    "**4) Evaluation**<br>\n",
    "Monitor the loss and the accuracy for the different epochs. For classification, generate a confusion chart and plot a histogramm of the different probabilities (see the lecture) at the end of the training process.<br>\n",
    "Evaluate the performance of the ANN with the test set in the same way.<br>\n",
    "<br>\n",
    "Now, experiment with different<br>\n",
    "- training to test set ratios<br>\n",
    "- different numbers of features in the data (say, $N_{feature} = 3, 5, 20, 50$)\n",
    "- features that correlate\n",
    "- different numbers of data points (say, $N_{sample} = 200, 2\\,000, 5\\,000, 10\\,000$)\n",
    "\n",
    "How does the accuracy change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6057e2-0c5b-4e79-9cae-b0afc98086f0",
   "metadata": {},
   "source": [
    "**5) Submission Requirements**<br>\n",
    "Include a short report (1-2 pages) explaining:<br>\n",
    "- your architecture choices (layers, neurons, activation functions, etc.)<br>\n",
    "- how dropout was implemented.<br>\n",
    "- training performance (loss and accuracy plots).<br>\n",
    "- results and key insights.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5664d",
   "metadata": {},
   "source": [
    "## Create artificial dataset function\n",
    "\n",
    "**1) Create an artificial dataset, one for regression and one for classification each** <br>\n",
    "Start with a data set similar to the molecule data set we have been using earlier with a moderate number of features (say five) and about 1000 data points.<br> \n",
    "Normalize the features between 0 and 1 for better convergence and split the dataset into **training and testing set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "133a942c-ae5e-437c-9c14-6ac41887b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from sklearn.preprocessing import MinMaxScaler  #to normalize data\n",
    "\n",
    "def create_dataset(num_data_points, num_features):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for num in range(num_features):\n",
    "        curr_column = np.random.randint(0, 100, size = num_data_points)\n",
    "        normalized_curr_col = MinMaxScaler().fit_transform(curr_column.reshape(-1,1)).flatten()\n",
    "        df[f\"feature {num+1}\"] = normalized_curr_col.flatten()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654f519",
   "metadata": {},
   "source": [
    "## make classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "659ef627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it a classification dataframe\n",
    "\n",
    "def make_class_df(df):\n",
    "    copy_df = df.copy()\n",
    "\n",
    "    columns = df.shape[1]\n",
    "\n",
    "    columns_list = np.arange(1, columns+1)\n",
    "\n",
    "    random_col = np.random.choice(columns_list, 2, replace = False)\n",
    "\n",
    "    toxic = (df[f\"feature {random_col[0]}\"] + df[f\"feature {random_col[1]}\"] > 1).astype(int)\n",
    "\n",
    "    copy_df['toxic'] = toxic\n",
    "\n",
    "    return copy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614aa77d",
   "metadata": {},
   "source": [
    "## make Train test split function for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c7241da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tt_split_regression(df, test_percent):\n",
    "\n",
    "    #make last feature Y\n",
    "\n",
    "    Y = df.iloc[:,-1]\n",
    "\n",
    "    X = df.iloc[:,:-1]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_percent )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989c98a",
   "metadata": {},
   "source": [
    "## Train test split function for classification df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37ccaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_split_classification(df, test_percent):\n",
    "\n",
    "    #make last feature Y\n",
    "\n",
    "    Y = df['toxic']\n",
    "\n",
    "    X = df.drop(columns = \"toxic\")\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_percent )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a2921",
   "metadata": {},
   "source": [
    "## Create regression and classification dataframes and do train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "90539cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## create regression dataframe\n",
    "rg_df_1 = create_dataset(1000, 5)\n",
    "\n",
    "#create classificaiton dataframe\n",
    "cl_df_1 = make_class_df(rg_df_1)\n",
    "#display(cl_df_1)\n",
    "#print(cl_df_1['toxic'])\n",
    "\n",
    "#train test split regression dataframe\n",
    "x_train_reg, x_test_reg, y_train_reg, y_test_reg = tt_split_regression(rg_df_1, 0.3)\n",
    "\n",
    "#train test split classification dataframe\n",
    "x_train_cla, x_test_cla, y_train_cla, y_test_cla = tt_split_regression(cl_df_1, 0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd74aff",
   "metadata": {},
   "source": [
    "## Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #gradients\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases  = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        self.dinputs  = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_ReLU():\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.maximum(0, inputs)\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0#ReLU derivative\n",
    "\n",
    "class Activation_Sigmoid():\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.clip(1/(1 + np.exp(-inputs)), 1e-7, 1-1e-7)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        sigm         = self.output\n",
    "        deriv        = np.multiply(sigm, (1 - sigm))#inner derivative of sigmoid\n",
    "        self.dinputs = np.multiply(deriv, dvalues)\n",
    "\n",
    "class Activation_Softmax:\n",
    "  \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values  = np.exp(inputs - np.max(inputs, axis = 1,\\\n",
    "                                      keepdims = True))#max in order to \n",
    "                                                       #prevent overflow\n",
    "        #normalizing probs (Boltzmann dist.)\n",
    "        probabilities = exp_values/np.sum(exp_values, axis = 1,\\\n",
    "                                      keepdims = True)  \n",
    "        self.output   = probabilities                                                \n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for i, (single_output, single_dvalues) in \\\n",
    "            enumerate(zip(self.output, dvalues)):\n",
    "            \n",
    "            single_output   = single_output.reshape(-1,1)\n",
    "            jacobMatr       = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            self.dinputs[i] = np.dot(jacobMatr, single_dvalues)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "     \n",
    "     def calculate(self, output, y):\n",
    "         \n",
    "         sample_losses = self.forward(output, y)\n",
    "         data_loss     = np.mean(sample_losses)\n",
    "         return(data_loss)\n",
    "    \n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss): \n",
    "\n",
    "     def forward(self, y_pred, y_true):\n",
    "         samples = len(y_pred)\n",
    "         #removing vals close to zero and one bco log and accuracy\n",
    "         y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "         \n",
    "         #now, depending on how classes are coded, we need to get the probs\n",
    "         if len(y_true.shape) == 1:#classes are encoded as [[1],[2],[2],[4]]\n",
    "             correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "         elif len(y_true.shape) == 2:#classes are encoded as\n",
    "                                    #[[1,0,0], [0,1,0], [0,1,0]]\n",
    "             correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
    "         #now: calculating actual losses\n",
    "         negative_log_likelihoods = -np.log(correct_confidences)\n",
    "         return(negative_log_likelihoods)\n",
    "         \n",
    "     def backward(self, dvalues, y_true):\n",
    "         Nsamples = len(dvalues)\n",
    "         Nlabels  = len(dvalues[0])\n",
    "         #turning labels into one-hot i. e. [[1,0,0], [0,1,0], [0,1,0]], if\n",
    "         #they are not\n",
    "         if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(Nlabels)[y_true]\n",
    "         #normalized gradient\n",
    "         self.dinputs = -y_true/dvalues/Nsamples\n",
    "\n",
    "\n",
    "\n",
    "#Creating a class as parent for softmax, loss and entropy classes. \n",
    "#Actually not neccessary, but saves code when building the ANN\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss       = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output#the probabilities\n",
    "        #calculates and returns mean loss\n",
    "        return(self.loss.calculate(self.output, y_true))\n",
    "        \n",
    "    def backward(self, dvalues, y_true):\n",
    "        Nsamples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculating normalized gradient\n",
    "        self.dinputs[range(Nsamples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs/Nsamples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    #initializing with a default learning rate of 0.01\n",
    "    def __init__(self, learning_rate = 0.01, decay = 0, momentum = 0):\n",
    "        self.learning_rate         = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay                 = decay\n",
    "        self.iterations            = 0\n",
    "        self.momentum              = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1/ (1 + self.decay*self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if we use momentum\n",
    "        if self.momentum:\n",
    "            \n",
    "            #check if layer has attribute \"momentum\"\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums   = np.zeros_like(layer.biases)\n",
    "                \n",
    "            #now the momentum parts\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates   = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases  += bias_updates\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Layer_dropout():\n",
    "    #source : https://www.youtube.com/watch?v=tkvlspCbLqo&ab_channel=Vizuara\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #generate scaled binary mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size = inputs.shape) / self.rate\n",
    "        #apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916b339",
   "metadata": {},
   "source": [
    "## Network Design\n",
    "\n",
    "Use at least two hidden layers and experiment with different layer sizes. Use different activation functions, such like *Sigmoid*, *ReLU* or any other activation function of your choice.<br>\n",
    "Implement dropout between the hidden layers (e.g., randomly drop 20-30% neurons).<br>\n",
    "Add an output layer according to the optimization problem (regression vs classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418570f4",
   "metadata": {},
   "source": [
    "## Regression ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "\n",
    "Nneurons1 = 64\n",
    "Nneurons2 = 128\n",
    "Nneurons3 = 256\n",
    "\n",
    "Nepochs = 2000\n",
    "learning_rate = 0.001\n",
    "decay = 0.001\n",
    "momentum = 0.8\n",
    "dropout_rate = 0.2\n",
    "\n",
    "Nfeatures = rg_df_1.shape[1] #columns in regression df\n",
    "\n",
    "dense1 = Layer_Dense(Nfeatures, Nneurons1)\n",
    "dense2 = Layer_Dense(Nneurons1, Nneurons2)\n",
    "dense_regression = Layer_Dense(Nneurons2, Nneurons3)\n",
    "\n",
    "drop1 = Layer_dropout(dropout_rate)\n",
    "drop2 = Layer_dropout(dropout_rate)\n",
    "\n",
    "\n",
    "optimizer = Optimizer_SGD(learning_rate = learning_rate, decay = decay, momentum = momentum)\n",
    "\n",
    "ReLU = Activation_ReLU()\n",
    "\n",
    "Target = y_train_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "## forward\n",
    "\n",
    "Monitor = np.zeros(Nepochs, 3) #stores loss, learning rate, accuracy\n",
    "\n",
    "for epoch in range(Nepochs):\n",
    "    dense1.forward(x_test_reg)\n",
    "    ReLU.forward(dense1.output)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde60ef",
   "metadata": {},
   "source": [
    "## classification ANN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msse-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
